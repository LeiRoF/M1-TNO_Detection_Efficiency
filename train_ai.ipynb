{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_multiplier = 5   # increase the size of the subset by taking random composition of CCD\n",
    "epoch = 60               # number of epochs for each AI\n",
    "n = 1                    # number of loops for the genertic algorithm\n",
    "K = 5                     # number of folds (= number of independant AI per generation)\n",
    "\n",
    "train_on = \"blocks\"     # \"triplets\" or \"blocks\"\n",
    "function = \"tan\"       # \"tan\" or \"square\"\n",
    "train_with = \"parameters\" # \"parameters\" or \"points\" TODO\n",
    "\n",
    "train_prop = 0.7          # proportion of the dataset used for training\n",
    "\n",
    "weights = [7.686424453317564,2.8124992994763316,0.448360122048755,4.727911837705288] # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, copy\n",
    "import data_io\n",
    "import utils.archive as archive\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import tensorflow        as tf\n",
    "from   IPython.display   import display\n",
    "from   tensorflow        import keras\n",
    "from   classes.block     import Block\n",
    "from   classes.triplet   import Triplet\n",
    "from   classes.shot      import Shot\n",
    "from   classes.ccd       import CCD\n",
    "from   classes.rate      import Rate\n",
    "from scipy.optimize      import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(Xt,Yt):\n",
    "    # mse = tf.keras.losses.MeanSquaredError()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input((len(Xt[0]),), name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(len(Yt[0]), name='Output'))\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of efficiency functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tan\n",
    "def ft(m,a,b,c,d):\n",
    "    return a/4 * (1-np.tanh((m-b)/c)) * (1-np.tanh((m-b)/d))\n",
    "\n",
    "# square\n",
    "def fs(m,a,b,c,d):\n",
    "    return (a-b*(m-21)**2) / (1+np.exp((m-c)/d))\n",
    "\n",
    "# magnitude range\n",
    "m = np.linspace(21,25.5,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2013AE.json\n",
      "Loading 2013AO.json\n",
      "Loading 2013BL.json\n",
      "Loading 2014BH.json\n",
      "Loading 2015AM.json\n",
      "Loading 2015AP.json\n",
      "Loading 2015BC.json\n",
      "Loading 2015BD.json\n",
      "Loading 2015BS.json\n",
      "Loading 2015BT.json\n",
      "2013AE\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\classes\\block.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  block_data = array(block_data)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 8 into shape (8,1444)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\train_ai.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000009?line=5'>6</a>\u001b[0m     data, outputs \u001b[39m=\u001b[39m data_io\u001b[39m.\u001b[39mget_ai_ready(items \u001b[39m=\u001b[39m Triplet\u001b[39m.\u001b[39mall, func\u001b[39m=\u001b[39mfunction,subsets_per_block\u001b[39m=\u001b[39msubset_multiplier, maxCCD\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39melif\u001b[39;00m train_on \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblocks\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000009?line=7'>8</a>\u001b[0m     data, outputs \u001b[39m=\u001b[39m data_io\u001b[39m.\u001b[39;49mget_ai_ready(items \u001b[39m=\u001b[39;49m Block\u001b[39m.\u001b[39;49mall, func\u001b[39m=\u001b[39;49mfunction,subsets_per_block\u001b[39m=\u001b[39;49msubset_multiplier, maxCCD\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000009?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtrain_on must be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtriplets\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mblocks\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\data_io.py:128\u001b[0m, in \u001b[0;36mget_ai_ready\u001b[1;34m(items, useExisting, func, vel, maxTriplet, maxCCD, randomTriplet, randomCCD, subsets_per_block)\u001b[0m\n\u001b[0;32m    126\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[39mwhile\u001b[39;00m i\u001b[39m<\u001b[39msubsets_per_block:\n\u001b[1;32m--> 128\u001b[0m     item_data, outputs \u001b[39m=\u001b[39m item\u001b[39m.\u001b[39;49mto_ai_ready(func \u001b[39m=\u001b[39;49m func, vel \u001b[39m=\u001b[39;49m vel, maxTriplet \u001b[39m=\u001b[39;49m maxTriplet, maxCCD \u001b[39m=\u001b[39;49m maxCCD, randomTriplet \u001b[39m=\u001b[39;49m randomTriplet, randomCCD \u001b[39m=\u001b[39;49m randomCCD)\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m item_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\classes\\block.py:58\u001b[0m, in \u001b[0;36mBlock.to_ai_ready\u001b[1;34m(self, func, vel, maxTriplet, maxCCD, randomTriplet, randomCCD, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     block_data\u001b[39m.\u001b[39mappend(triplet_data)\n\u001b[0;32m     57\u001b[0m block_data \u001b[39m=\u001b[39m array(block_data)\n\u001b[1;32m---> 58\u001b[0m \u001b[39mtry\u001b[39;00m: block_data \u001b[39m=\u001b[39m block_data\u001b[39m.\u001b[39;49mreshape(\u001b[39mlen\u001b[39;49m(block_data),\u001b[39mlen\u001b[39;49m(triplet_data))\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 8 into shape (8,1444)"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "data_io.loadAll() # comment if already loaded to spare time\n",
    "\n",
    "# Formating data in data set usable by the AI\n",
    "if train_on == \"triplets\":\n",
    "    data, outputs = data_io.get_ai_ready(items = Triplet.all, func=function,subsets_per_block=subset_multiplier, maxCCD=30)\n",
    "elif train_on == \"blocks\":\n",
    "    data, outputs = data_io.get_ai_ready(items = Block.all, func=function,subsets_per_block=subset_multiplier, maxCCD=30)\n",
    "else:\n",
    "    raise ValueError(\"train_on must be 'triplets' or 'blocks'\")\n",
    "\n",
    "print(len(data), \"vectors containing\", len(data[0])-outputs, \"inputs and\", outputs, \"outputs\")\n",
    "\n",
    "# Normalization\n",
    "mean = data[:,:-outputs].mean()\n",
    "std  = data[:,:-outputs].std()\n",
    "data[:,:-outputs] = (data[:,:-outputs] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triplet in Block.all[\"2013AE\"].tripletList:\n",
    "    print(triplet.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in Block.all.values():\n",
    "    if block.rates == []:\n",
    "        print(\"No rates for block\", block.id)\n",
    "        continue\n",
    "    if len(block.rates) > 1: continue\n",
    "\n",
    "    print('\\nBlock', block.id, end=\": \")\n",
    "    for rate in block.rates:\n",
    "        print(rate.func, end=\", \")\n",
    "\n",
    "    if block.rates[0].func == \"tan\":\n",
    "        param = [0.5, 0.5, 25, 0.5]\n",
    "        param, pcov = curve_fit(fs, m, ft(m,block.rates[0].a, block.rates[0].b, block.rates[0].c, block.rates[0].d), param)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(m, ft(m,block.rates[0].a, block.rates[0].b, block.rates[0].c, block.rates[0].d), label=\"data (tan)\")\n",
    "        plt.plot(m, fs(m,*param), label=\"fit (square)\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.title(\"Block \"+str(block.id))\n",
    "\n",
    "    if block.rates[0].func == \"square\":\n",
    "        param = [0.5, 25, 0.5, 0.5]\n",
    "        param, pcov = curve_fit(ft, m, fs(m,block.rates[0].a, block.rates[0].b, block.rates[0].c, block.rates[0].d), param)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(m, fs(m,block.rates[0].a, block.rates[0].b, block.rates[0].c, block.rates[0].d), label=\"data (square)\")\n",
    "        plt.plot(m, ft(m,*param), label=\"fit (tan)\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.title(\"Block \"+str(block.id))\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a random test item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_item():\n",
    "    if train_on == \"blocks\":\n",
    "        test_item = np.array(list(Block.all.values()))[np.random.randint(0,len(Block.all)-1)]\n",
    "\n",
    "    if train_on == \"triplets\":\n",
    "        tripletList = []\n",
    "        for rate in Rate.all:\n",
    "            if type(rate.parent) == Triplet and rate.parent.id not in tripletList:\n",
    "                tripletList.append(rate.parent.id)\n",
    "        test_item = Triplet.all[tripletList[np.random.randint(0,len(tripletList)-1)]]\n",
    "\n",
    "    print(\"Test item:\",test_item.id)\n",
    "\n",
    "    return test_item\n",
    "\n",
    "test_item = get_test_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_data(test_item):\n",
    "    data,outputs = test_item.to_ai_ready(func=function,maxCCD=30)\n",
    "\n",
    "    X = data[:-outputs]\n",
    "    Y = data[-outputs:]\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    return X , Y\n",
    "\n",
    "def predict(test_item, model):\n",
    "\n",
    "    X, Y = item_data(test_item)\n",
    "\n",
    "    X=np.array(X).reshape(1,len(X))\n",
    "\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    print(f\"Prediction : {prediction[0]}\")\n",
    "\n",
    "    return prediction[0]\n",
    "\n",
    "def plot_prediction(test_item, prediction, i = 0, n = 1):\n",
    "\n",
    "    X, Y = item_data(test_item)\n",
    "\n",
    "    # Plotting the result\n",
    "    f = ft if function == \"tan\" else fs\n",
    "\n",
    "    plt.subplot(int(np.ceil(np.sqrt(n))),int(np.ceil(np.sqrt(n))),i+1)\n",
    "    plt.plot(m, f(m,*prediction),         label=\"Machine Learning\")\n",
    "    plt.plot(m, f(m,Y[0],Y[1],Y[2],Y[3]), label=\"Excpected\")\n",
    "    plt.grid()\n",
    "\n",
    "    if i == 0:                    plt.title(f\"Predition for {train_on} {test_item.id}\")\n",
    "    if i+1>n-np.ceil(np.sqrt(n)): plt.xlabel(\"Magnitude\")\n",
    "    if i%np.ceil(np.sqrt(n))==0:  plt.ylabel(\"Efficiency\")\n",
    "    if i==0:                      plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenerating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(K, data, train_prop, outputs):\n",
    "\n",
    "    # Creating folds\n",
    "    folds = []\n",
    "    for j in range(K):\n",
    "        folds.append(data[j::K])\n",
    "\n",
    "    # Splitting data for training and test...\n",
    "    Xt_list = []; Yt_list = []; Xv_list = []; Yv_list = []\n",
    "    for j, fold in enumerate(folds):\n",
    "        train_sets = int(len(fold)*train_prop)\n",
    "        index = np.zeros(len(fold),dtype=bool)\n",
    "        index[:train_sets] = True\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        data_train = fold[index]\n",
    "        data_test  = fold[~index]\n",
    "\n",
    "        Xt_list.append(data_train[:,:-outputs])\n",
    "        Yt_list.append(data_train[:,-outputs:])\n",
    "        Xv_list.append(data_test [:,:-outputs])\n",
    "        Yv_list.append(data_test [:,-outputs:])\n",
    "\n",
    "    if K > 1:\n",
    "        # Composing training data using fold != j\n",
    "        for j in range(K):\n",
    "            Xt_list[j] = np.concatenate(np.array(Xt_list)[np.arange(len(Xt_list))!=j])\n",
    "            Yt_list[j] = np.concatenate(np.array(Yt_list)[np.arange(len(Yt_list))!=j])\n",
    "\n",
    "    return np.array(Xt_list), np.array(Yt_list), np.array(Xv_list), np.array(Yv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, Xt,Yt,Xv,Yv,epoch):\n",
    "    h = model.fit(Xt, Yt, epochs = epoch, verbose = 0)        #, validation_data = (x_test[j], y_test[j])\n",
    "    s = model.evaluate(Xv, Yv, verbose=0)\n",
    "    return h,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping only the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(generation, models, scores, lastScore, bestModel):\n",
    "    minScore = lastScore\n",
    "    output.write(f\"{generation+1}, \")\n",
    "    if minScore is None: minScore = scores[0][0]\n",
    "    if bestModel is None: bestModel = models[0]\n",
    "    average = 0\n",
    "    for j,s in enumerate(scores):\n",
    "        output.write(f\"{s[0]}, \") \n",
    "        average += s[0]/K\n",
    "        if s[0] < minScore:\n",
    "            minScore = s[0]\n",
    "            bestModel = models[j]\n",
    "    output.write(f\"{average}, {minScore}\\n\")\n",
    "    return bestModel, minScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural nework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = None\n",
    "bestScore = None\n",
    "path = archive.new(name = archive.description(M = subset_multiplier, N = n, K = K, E = epoch, F = function, T = train_on, Y = train_with , P = train_prop))\n",
    "output = open(f\"{path}/ouput.csv\",\"w\")\n",
    "\n",
    "output.write(\"Generation, \")\n",
    "for j in np.arange(K): output.write(f\"Score of model {j}, \")\n",
    "output.write(\"Average, Score retained\\n\")\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "# Loop over generations (genetic algorithm)\n",
    "for i in range(n):\n",
    "  \n",
    "  print(f\"🔁 Generation {i+1}/{n}\")\n",
    "\n",
    "  np.random.shuffle(data)\n",
    "\n",
    "  Xt_list, Yt_list, Xv_list, Yv_list = create_folds(K, data, train_prop, outputs)\n",
    "  \n",
    "\n",
    "  ################################################################################\n",
    "  # Training K models independently\n",
    "\n",
    "  models = []; history = []; scores = []\n",
    "  for j in range(K):\n",
    "\n",
    "    if K > 1:\n",
    "      print(f\"🏃‍♀️ Training model  with fold {j+1} as test...\", end=\"\\r\")\n",
    "      if j+1 == K: print(f\"🏃‍♀️ Training model  with fold {j+1} as test...\")\n",
    "\n",
    "    # Sub sets for this fold\n",
    "\n",
    "    Xt, Yt, Xv, Yv = Xt_list[j], Yt_list[j], Xv_list[j], Yv_list[j]\n",
    "\n",
    "    # Getting new model if it's the first generation, and the old one if not\n",
    "\n",
    "    if len(models) < j+1: models.append(get_model(Xt,Yt))\n",
    "    else: models.append(copy.deepcopy(bestModel))\n",
    "\n",
    "    model = models[j]\n",
    "\n",
    "\n",
    "    # Training models\n",
    "\n",
    "    res, score = train(model, Xt, Yt, Xv, Yv, epoch)\n",
    "    history.append(res)\n",
    "    scores.append(score)\n",
    "\n",
    "  # Keeping the best one\n",
    "\n",
    "  bestModel, bestScore = get_best_model(i,models, scores, bestScore, bestModel)\n",
    "\n",
    "  # Making new prediction\n",
    "\n",
    "  print(\"\\n🔮 Prediction...\")\n",
    "  prediction = predict(test_item, bestModel)\n",
    "  plot_prediction(test_item, prediction, i, n)\n",
    "\n",
    "################################################################################\n",
    "# Saving results\n",
    "\n",
    "output.close()\n",
    "model.save(f\"{path}/model.ckpt\")\n",
    "plt.savefig(f\"{path}/tno_efficiency_rate.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "N = 25\n",
    "for i in range(N):\n",
    "    test_item = get_test_item()\n",
    "    prediction = predict(test_item, bestModel)\n",
    "    plot_prediction(test_item, prediction, i, N)\n",
    "\n",
    "plt.savefig(f\"{path}/prediction.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0d398e7c3cc3d2b8386dfea47f5eae3378d5d39db7e2c8ef87e93246db0bfd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
